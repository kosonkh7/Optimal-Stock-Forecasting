{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86b39746-d051-4927-9f4a-b5a967e15e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.torch.model.deepar import DeepAREstimator\n",
    "from gluonts.dataset.split import split\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from pytorch_lightning import Trainer\n",
    "import torch\n",
    "from gluonts.torch.distributions import StudentTOutput\n",
    "\n",
    "import time # time 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28c402e6-619a-4f7c-a930-88d39f48de5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\PC\\anaconda3\\envs\\llm\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type        | Params | Mode  | In sizes                                                          | Out sizes    \n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "0 | model | DeepARModel | 25.9 K | train | [[1, 1], [1, 1], [1, 1122, 4], [1, 1122], [1, 1122], [1, 365, 4]] | [1, 100, 365]\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "25.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.9 K    Total params\n",
      "0.104     Total estimated model params size (MB)\n",
      "11        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfc4aedbcaf4027bd34fa1db990a3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'train_loss' reached 7.68664 (best 7.68664), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=0-step=50.ckpt' as top 1\n",
      "Epoch 1, global step 100: 'train_loss' reached 7.23511 (best 7.23511), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=1-step=100.ckpt' as top 1\n",
      "Epoch 2, global step 150: 'train_loss' reached 6.53479 (best 6.53479), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=2-step=150.ckpt' as top 1\n",
      "Epoch 3, global step 200: 'train_loss' reached 6.25116 (best 6.25116), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=3-step=200.ckpt' as top 1\n",
      "Epoch 4, global step 250: 'train_loss' reached 6.07980 (best 6.07980), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=4-step=250.ckpt' as top 1\n",
      "Epoch 5, global step 300: 'train_loss' reached 6.06262 (best 6.06262), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=5-step=300.ckpt' as top 1\n",
      "Epoch 6, global step 350: 'train_loss' reached 5.91656 (best 5.91656), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=6-step=350.ckpt' as top 1\n",
      "Epoch 7, global step 400: 'train_loss' reached 5.84841 (best 5.84841), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=7-step=400.ckpt' as top 1\n",
      "Epoch 8, global step 450: 'train_loss' reached 5.79061 (best 5.79061), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=8-step=450.ckpt' as top 1\n",
      "Epoch 9, global step 500: 'train_loss' reached 5.72857 (best 5.72857), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=9-step=500.ckpt' as top 1\n",
      "Epoch 10, global step 550: 'train_loss' reached 5.67574 (best 5.67574), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=10-step=550.ckpt' as top 1\n",
      "Epoch 11, global step 600: 'train_loss' reached 5.63547 (best 5.63547), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=11-step=600.ckpt' as top 1\n",
      "Epoch 12, global step 650: 'train_loss' reached 5.60308 (best 5.60308), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=12-step=650.ckpt' as top 1\n",
      "Epoch 13, global step 700: 'train_loss' reached 5.56927 (best 5.56927), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=13-step=700.ckpt' as top 1\n",
      "Epoch 14, global step 750: 'train_loss' reached 5.54149 (best 5.54149), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=14-step=750.ckpt' as top 1\n",
      "Epoch 15, global step 800: 'train_loss' reached 5.50420 (best 5.50420), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=15-step=800.ckpt' as top 1\n",
      "Epoch 16, global step 850: 'train_loss' reached 5.47471 (best 5.47471), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=16-step=850.ckpt' as top 1\n",
      "Epoch 17, global step 900: 'train_loss' reached 5.44288 (best 5.44288), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=17-step=900.ckpt' as top 1\n",
      "Epoch 18, global step 950: 'train_loss' reached 5.41374 (best 5.41374), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=18-step=950.ckpt' as top 1\n",
      "Epoch 19, global step 1000: 'train_loss' reached 5.39044 (best 5.39044), saving model to 'C:\\\\Users\\\\PC\\\\Desktop\\\\demand_prediction\\\\lightning_logs\\\\version_22\\\\checkpoints\\\\epoch=19-step=1000.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== (F) final_df 미리보기 ===\n",
      "        date  count       predict\n",
      "0 2023-01-01     97           NaN\n",
      "1 2023-01-02   1316 -4.494333e-11\n",
      "2 2023-01-03   1020 -3.401029e-11\n",
      "3 2023-01-04   1025 -2.877042e-11\n",
      "4 2023-01-05    972 -4.804887e-11\n",
      "5 2023-01-06    921 -4.388564e-11\n",
      "6 2023-01-07    212 -4.866495e-11\n",
      "7 2023-01-08    246 -4.508920e-11\n",
      "8 2023-01-09   1777 -3.954440e-11\n",
      "9 2023-01-10   1548 -3.231463e-11\n",
      "          date  count       predict\n",
      "355 2023-12-22    747 -3.977127e-11\n",
      "356 2023-12-23    119 -4.428961e-11\n",
      "357 2023-12-24     22 -4.883894e-11\n",
      "358 2023-12-25    219 -4.219178e-11\n",
      "359 2023-12-26   1442 -4.189639e-11\n",
      "360 2023-12-27    966 -4.195521e-11\n",
      "361 2023-12-28    822 -4.485171e-11\n",
      "362 2023-12-29    717 -6.883579e-11\n",
      "363 2023-12-30     88 -3.757549e-11\n",
      "364 2023-12-31     21 -4.093062e-11\n",
      "285.8772 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time() # 시작\n",
    "\n",
    "# 2) 데이터 로드 및 전처리\n",
    "logistics = pd.read_csv(\"logistics_dong2.csv\", encoding=\"euc-kr\", parse_dates=[\"배송년월일\"])\n",
    "target_area = '논현1동'\n",
    "target_category = '식품r'\n",
    "\n",
    "logistics['배송년월일'] = pd.to_datetime(logistics['배송년월일'], format='%Y%m%d')\n",
    "data = logistics.loc[logistics.행정동명==target_area][['배송년월일', target_category]]\n",
    "data.columns = ['date', 'count']\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "df = data[['date', 'count']]\n",
    "\n",
    "# 2) Train/Test 날짜 기준으로 직접 분할\n",
    "cutoff_date = pd.Timestamp(\"2023-01-01\")\n",
    "\n",
    "train_df = df[df[\"date\"] < cutoff_date].copy()\n",
    "test_df  = df[df[\"date\"] >= cutoff_date].copy()\n",
    "\n",
    "# 3) PandasDataset 생성\n",
    "train_dataset = PandasDataset(\n",
    "    train_df, \n",
    "    target=\"count\", \n",
    "    timestamp=\"date\", \n",
    "    freq=\"D\"\n",
    ")\n",
    "\n",
    "# test_dataset = PandasDataset(\n",
    "#     test_df, \n",
    "#     target=\"count\", \n",
    "#     timestamp=\"date\", \n",
    "#     freq=\"D\"\n",
    "# )\n",
    "\n",
    "# 4) DeepAREstimator 생성\n",
    "estimator = DeepAREstimator(\n",
    "    freq=\"D\",  # 데이터의 시간 단위\n",
    "    prediction_length=7,  # 예측 길이: 7일\n",
    "    context_length=30,  # 문맥 길이: 30일\n",
    "    num_layers=3,  # RNN 계층 수\n",
    "    hidden_size=50,  # RNN 셀 크기\n",
    "    lr=0.001,  # 학습률\n",
    "    dropout_rate=0.1,  # 드롭아웃 비율\n",
    "    patience=5,\n",
    "    num_feat_dynamic_real=0 # len(feat_dynamic),  # 동적 실수형 특성의 수\n",
    "    num_feat_static_cat=0,  # 정적 범주형 특성의 수\n",
    "    scaling=True,  # 스케일링 활성화\n",
    "    batch_size=64,  # 배치 크기\n",
    "    num_parallel_samples=200,  # 병렬 샘플 수\n",
    "    trainer_kwargs={\n",
    "        \"max_epochs\": 5,  # 최대 에포크 수\n",
    "        \"accelerator\": \"gpu\" if torch.cuda.is_available() else \"cpu\",  # CPU 또는 GPU 선택\n",
    "        \"devices\": 1 if torch.cuda.is_available() else None  # 사용할 장치 수 설정\n",
    "    }\n",
    ")\n",
    "\n",
    "# 5) 학습\n",
    "predictor = estimator.train(train_dataset)\n",
    "\n",
    "# ======================================================================\n",
    "# (E) \"2023-01-01\"만 포함된 더미 test dataset 으로 1년 예측\n",
    "# ======================================================================\n",
    "one_shot_test_df = pd.DataFrame({\n",
    "    \"date\":  [pd.Timestamp(\"2023-01-01\")],\n",
    "    \"count\": [0]  # 의미 없는 placeholder\n",
    "})\n",
    "\n",
    "test_dataset = PandasDataset(\n",
    "    one_shot_test_df,\n",
    "    target=\"count\",\n",
    "    timestamp=\"date\",\n",
    "    freq=\"D\"\n",
    ")\n",
    "\n",
    "forecasts = list(predictor.predict(test_dataset))\n",
    "if len(forecasts) == 0:\n",
    "    raise RuntimeError(\"예측 생성 실패: test_dataset이 잘못되었거나 모델 예측에 문제가 있습니다.\")\n",
    "\n",
    "forecast = forecasts[0]  # 단일 시계열 → 1개 forecast\n",
    "# forecast.start_date = 2023-01-01 (기본적으로)\n",
    "# 예측 길이 = 365일\n",
    "\n",
    "# 중앙값(quantile=0.5)으로 시계열 얻기\n",
    "pred_series = forecast.quantile(0.5)\n",
    "pred_dates = [forecast.start_date + pd.Timedelta(days=i) for i in range(len(pred_series))]\n",
    "\n",
    "df_pred = pd.DataFrame({\"date\": pred_dates, \"predict\": pred_series})\n",
    "df_pred[\"date\"] = df_pred[\"date\"].dt.to_timestamp()\n",
    "\n",
    "# ======================================================================\n",
    "# (F) 실제값 (2023년)과 예측값 머지\n",
    "# ======================================================================\n",
    "# test_df(2023-01-01~2023-12-31)에서 실제 count를 가져옴\n",
    "df_2023 = test_df.copy()  # ['date','count']\n",
    "\n",
    "# date 기준으로 머지\n",
    "final_df = pd.merge(df_2023, df_pred, on=\"date\", how=\"left\")\n",
    "# 만약 test_df에 2023-12-31까지, df_pred도 2023-12-31까지 있을 것\n",
    "# right join 하므로 예측 날짜가 우선 -> 실제값이 없는 날짜면 NaN이 들어감\n",
    "\n",
    "final_df = final_df[[\"date\", \"count\", \"predict\"]].sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== (F) final_df 미리보기 ===\")\n",
    "print(final_df.head(10))\n",
    "print(final_df.tail(10))\n",
    "\n",
    "\n",
    "print(f\"{time.time()-start:.4f} sec\") # 종료와 함께 수행시간 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f33d5-069a-4928-8bc7-5706a57fb374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c765656d-644e-400d-97ee-ea8fa53dc7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac44fce-cc4e-4be8-9dec-0470e8b58bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c95dbfb-9dcc-4ba7-9993-b0b530e71f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83409e66-1466-49e1-a51e-d054aa26c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_to_2023(input_date_str: str) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    입력 날짜가 어떤 연도든, '월/일'만 유지하고 연도는 2023으로 치환.\n",
    "    예) '2025-01-20' -> 2023-01-20\n",
    "    \"\"\"\n",
    "    parsed = pd.to_datetime(input_date_str, errors=\"raise\")\n",
    "    forced_2023 = parsed.replace(year=2023)  # 월일 그대로, 연도=2023으로 변경\n",
    "    return forced_2023\n",
    "\n",
    "def get_7day_forecast(input_date_str: str, df_2023: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    - input_date_str: 사용자 입력 (연도 무관, 예: '2025-01-20', '03/05' 등)\n",
    "    - df_2023: 'date', 'count', 'predict' (2023년 1년치) 데이터프레임\n",
    "    \n",
    "    returns (target_date(2023), actual_value, predict_value)\n",
    "     * target_date = (입력일을 2023년으로 치환) + 7일\n",
    "     * actual_value = 그 날짜의 실제 수요(df_2023.count) / 없으면 None\n",
    "     * predict_value = 그 날짜의 예측 수요(df_2023.predict) / 없으면 None\n",
    "    \"\"\"\n",
    "    base_date_2023 = parse_to_2023(input_date_str)\n",
    "    target_date = base_date_2023 + timedelta(days=7)\n",
    "\n",
    "    row = df_2023.loc[df_2023[\"date\"] == target_date]\n",
    "    if row.empty:\n",
    "        return (target_date, None, None)\n",
    "\n",
    "    actual_value = row[\"count\"].values[0]   # 실제\n",
    "    predict_value = row[\"predict\"].values[0]  # 예측\n",
    "    return (target_date, actual_value, predict_value)\n",
    "\n",
    "# ======================================================================\n",
    "\n",
    "# ======================================================================\n",
    "input_examples = [\"2023-03-05\", \"2025-01-20\", \"2025/07/01\", \"12-25\"]\n",
    "for d_str in input_examples:\n",
    "    td, act, pred = get_7day_forecast(d_str, final_df)\n",
    "    print(\"\\n입력:\", d_str, \"-> 2023로 치환 후 +7일 =\", td.date())\n",
    "    print(\"실제수요 =\", act, \"/ 예측수요 =\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e02adc-c3ed-43a5-b6c0-9a366e335acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 예측 및 평가\n",
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=test_dataset,\n",
    "    predictor=predictor,\n",
    "    num_samples=100\n",
    ")\n",
    "\n",
    "forecasts = list(forecast_it)\n",
    "tss = list(ts_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da442c-fdcf-48e8-808a-d89399d0b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 평가 지표 확인\n",
    "evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
    "agg_metrics, item_metrics = evaluator(tss, forecasts, num_series=len(test_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
